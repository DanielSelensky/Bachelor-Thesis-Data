{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mY_tmNlxMdo"
      },
      "outputs": [],
      "source": [
        "!pip install PyMuPDF pymongo PyDrive2\n",
        "\n",
        "!pip install langchain langchain_community langchain-openai langsmith pytesseract openai\n",
        "!sudo apt install tesseract-ocr tesseract-ocr-deu\n",
        "!pip install bert-score rouge-score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transforming PDF into individual images:"
      ],
      "metadata": {
        "id": "dGVvTMiu1s8m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "from PIL import Image\n",
        "from google.colab import drive\n",
        "from google.colab import userdata\n",
        "\n",
        "import io\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "from typing import List, Union, Tuple\n",
        "\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "LANGCHAIN_API_KEY = userdata.get('LANGCHAIN_API')\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"Bachelor Thesis\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = LANGCHAIN_API_KEY\n",
        "\n",
        "from langsmith import Client\n",
        "\n",
        "langsmith_client = Client()"
      ],
      "metadata": {
        "id": "YmsPYVMknzK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.agents import tool, AgentExecutor\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.prompts.chat import MessagesPlaceholder\n",
        "from langchain_core.messages.system import SystemMessage\n",
        "from langchain_core.messages.human import HumanMessage\n",
        "from langchain.agents.format_scratchpad.openai_tools import (\n",
        "    format_to_openai_tool_messages,\n",
        ")\n",
        "from langchain.agents.output_parsers.openai_tools import OpenAIToolsAgentOutputParser\n",
        "\n",
        "from typing import Optional\n",
        "\n",
        "import json\n",
        "\n",
        "llm = ChatOpenAI(api_key=OPENAI_API_KEY, model=\"gpt-4o\")"
      ],
      "metadata": {
        "id": "4A4Paz81jpBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "vTe8b7ZYoEQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pdf_to_images(pdf_path, output_folder):\n",
        "    # Open the PDF file\n",
        "    pdf_document = fitz.open(pdf_path)\n",
        "    number_of_pages = pdf_document.page_count\n",
        "\n",
        "    # Loop through each page\n",
        "    for page_num in range(number_of_pages):\n",
        "        # Get the page\n",
        "        page = pdf_document.load_page(page_num)\n",
        "        # Render page to an image\n",
        "        pix = page.get_pixmap(dpi=200)\n",
        "\n",
        "        # Convert to a PIL image\n",
        "        img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
        "\n",
        "        # Save the image\n",
        "        img.save(f\"{output_folder}/page_{page_num + 1}.png\")"
      ],
      "metadata": {
        "id": "ZBe7tfpZoaQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_to_images(\"/content/drive/MyDrive/bachelor-thesis/pdfs/01.pdf\", \"/content/drive/MyDrive/bachelor-thesis/output-images\")"
      ],
      "metadata": {
        "id": "A-bf6UXWog2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create description from image, embed it and store in MongoDB vector store"
      ],
      "metadata": {
        "id": "1Bc7ZxrG15Rv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_image_paths(directory: str, number: int = None) -> List[str]:\n",
        "    image_paths = []\n",
        "    count = 0\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith('.png'):\n",
        "            image_paths.append(os.path.join(directory, filename))\n",
        "            if number is not None and count == number:\n",
        "                return [image_paths[-1]]\n",
        "            count += 1\n",
        "    return image_paths\n",
        "direc = '/content/drive/MyDrive/bachelor-thesis/output-images/'\n",
        "image_paths = get_image_paths(direc)\n",
        "image_paths"
      ],
      "metadata": {
        "id": "Nh9tzPR6Yb4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tools for Agent:"
      ],
      "metadata": {
        "id": "NE4t673jkAGI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tool\n",
        "def get_diagrams_from_slide(imgurl):\n",
        "  \"\"\"Returns the types of diagrams and charts and their topic in a image for a given URL. Must use this, if the image has charts or diagrams in it.\"\"\"\n",
        "  prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You will be given an image. Your task is to identify the types of diagrams or charts used in the image. Only return the titles of the chart or diagram and their type. Examples for such charts can be pie charts, bar charts, line graphs, histograms, ...\"),\n",
        "    (\"human\", [\n",
        "              {\n",
        "                  \"type\": \"image_url\",\n",
        "                  \"image_url\": {\n",
        "                      \"url\": \"{imgurl}\"\n",
        "                      }\n",
        "                  }\n",
        "              ]),\n",
        "  ])\n",
        "  prompt_value = prompt.invoke({\"imgurl\": imgurl})\n",
        "  try:\n",
        "    answer = llm.invoke(prompt_value)\n",
        "  except:\n",
        "    print(\"Error in get_diagrams_from_slide. Trying again\")\n",
        "    answer = get_diagrams_from_slide(imgurl)\n",
        "  return answer\n",
        "\n",
        "@tool\n",
        "def describe_bar_chart(imgurl, diagram_info, bars_count, ocr_text):\n",
        "  \"\"\"Returns a detailed description of a given bar chart from an image. Use this tool when the amount of bars is known. Always use this tool if the image contains a bar chart.\"\"\"\n",
        "  prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You will be given an image. Your task is to describe the bar chart given in the image. If asked about a specific part of the image, only return the information in the specific part. Use the information about the amount of bars to make sure you describe every bar in detail.\"),\n",
        "    (\"human\", [\n",
        "        {\n",
        "                  \"type\": \"text\",\n",
        "                  \"text\": \"\"\"Focus on the following chart: {diagram_info}\n",
        "                  There is the following amount of bars in the chart: {bars_count}\n",
        "\n",
        "                  The following text is in the image: {ocr_text}\"\"\"\n",
        "                  },\n",
        "              {\n",
        "                  \"type\": \"image_url\",\n",
        "                  \"image_url\": {\n",
        "                      \"url\": \"{imgurl}\"\n",
        "                      }\n",
        "                  }\n",
        "              ]),\n",
        "  ])\n",
        "  title = diagram_info\n",
        "  if(isinstance(diagram_info, dict)):\n",
        "    if \"topic\" in diagram_info:\n",
        "      title = diagram_info[\"topic\"]\n",
        "  prompt_value = prompt.invoke({\"imgurl\": imgurl, \"diagram_info\": title, \"bars_count\": bars_count, \"ocr_text\": ocr_text})\n",
        "  try:\n",
        "    answer = llm.invoke(prompt_value)\n",
        "  except:\n",
        "    print(\"Error in describe_bar_chart. Trying again\")\n",
        "    answer = describe_bar_chart(imgurl, diagram_info, bars_count, ocr_text)\n",
        "  return answer\n",
        "\n",
        "@tool\n",
        "def describe_pie_chart(imgurl, diagram_info, ocr_text, color_meaning: Optional[str] = None):\n",
        "  \"\"\"Returns a detailed description of a given pie chart from an image. Always use this tool when the image contains a pie chart.\"\"\"\n",
        "  if color_meaning is None:\n",
        "    color_meaning = \"None\"\n",
        "  prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You will be given an image. Your task is to describe the pie chart given in the image. If asked about a specific part of the image, only return the information in the specific part.\"),\n",
        "    (\"human\", [\n",
        "        {\n",
        "                  \"type\": \"text\",\n",
        "                  \"text\": \"\"\"Focus on the following chart: {diagram_info}\n",
        "\n",
        "                  The following information on the meaning of the colors of the slices is known: {color_meaning}\n",
        "\n",
        "                  The following text is in the image: {ocr_text}\"\"\"\n",
        "                  },\n",
        "              {\n",
        "                  \"type\": \"image_url\",\n",
        "                  \"image_url\": {\n",
        "                      \"url\": \"{imgurl}\"\n",
        "                      }\n",
        "                  }\n",
        "              ]),\n",
        "  ])\n",
        "  title = diagram_info\n",
        "  if(isinstance(diagram_info, dict)):\n",
        "    if \"topic\" in diagram_info:\n",
        "      title = diagram_info[\"topic\"]\n",
        "  prompt_value = prompt.invoke({\"imgurl\": imgurl, \"diagram_info\": title, \"color_meaning\": color_meaning, \"ocr_text\": ocr_text})\n",
        "  try:\n",
        "    answer = llm.invoke(prompt_value)\n",
        "  except:\n",
        "    print(\"Error in describe_pie_chart. Trying again\")\n",
        "    answer = describe_pie_chart(imgurl, diagram_info, ocr_text, color_meaning)\n",
        "  return answer\n",
        "\n",
        "@tool\n",
        "def describe_timeline_chart(imgurl, diagram_info, ocr_text):\n",
        "  \"\"\"Returns a detailed description of a given timeline chart from an image. Always use this tool when the image contains a timeline chart.\"\"\"\n",
        "  prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You will be given an image. Your task is to describe the timeline chart given in the image. If asked about a specific part of the image, only return the information in the specific part. Describe with as detail as possible, describing each point in time.\"),\n",
        "    (\"human\", [\n",
        "        {\n",
        "                  \"type\": \"text\",\n",
        "                  \"text\": \"\"\"Focus on the following chart: {diagram_info}\n",
        "\n",
        "                  The following text is in the image: {ocr_text}\"\"\"\n",
        "                  },\n",
        "              {\n",
        "                  \"type\": \"image_url\",\n",
        "                  \"image_url\": {\n",
        "                      \"url\": \"{imgurl}\"\n",
        "                      }\n",
        "                  }\n",
        "              ]),\n",
        "  ])\n",
        "  title = diagram_info\n",
        "  if(isinstance(diagram_info, dict)):\n",
        "    if \"topic\" in diagram_info:\n",
        "      title = diagram_info[\"topic\"]\n",
        "  prompt_value = prompt.invoke({\"imgurl\": imgurl, \"diagram_info\": title, \"ocr_text\": ocr_text})\n",
        "  try:\n",
        "    answer = llm.invoke(prompt_value)\n",
        "  except:\n",
        "    print(\"Error in describe_timeline_chart. Trying again\")\n",
        "    answer = describe_timeline_chart(imgurl, diagram_info, ocr_text)\n",
        "  return answer\n",
        "\n",
        "@tool\n",
        "def describe_table(imgurl, diagram_info, ocr_text):\n",
        "  \"\"\"Returns a detailed description of a given table from an image. Always use this tool when the image contains a table.\"\"\"\n",
        "  prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You will be given an image. Your task is to describe the table given in the image. If asked about a specific part of the image, only return the information in the specific part. Describe with as detail as possible, describing each entry.\"),\n",
        "    (\"human\", [\n",
        "        {\n",
        "                  \"type\": \"text\",\n",
        "                  \"text\": \"\"\"Focus on the following table: {diagram_info}\n",
        "\n",
        "                  The following text is in the image: {ocr_text}\"\"\"\n",
        "                  },\n",
        "              {\n",
        "                  \"type\": \"image_url\",\n",
        "                  \"image_url\": {\n",
        "                      \"url\": \"{imgurl}\"\n",
        "                      }\n",
        "                  }\n",
        "              ]),\n",
        "  ])\n",
        "  title = diagram_info\n",
        "  if(isinstance(diagram_info, dict)):\n",
        "    if \"topic\" in diagram_info:\n",
        "      title = diagram_info[\"topic\"]\n",
        "  prompt_value = prompt.invoke({\"imgurl\": imgurl, \"diagram_info\": title, \"ocr_text\": ocr_text})\n",
        "  try:\n",
        "    answer = llm.invoke(prompt_value)\n",
        "  except:\n",
        "    print(\"Error in describe_table. Trying again\")\n",
        "    answer = describe_table(imgurl, diagram_info, ocr_text)\n",
        "  return answer\n",
        "\n",
        "@tool\n",
        "def describe_architecture_flowchart_diagram(imgurl, diagram_info, ocr_text):\n",
        "  \"\"\"Returns a detailed description of a software architecture diagram. This can be in the form of a flowchart diagram that shows a technical architecture of a software system. Use this tool only for software architecture diagrams.\"\"\"\n",
        "  prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You will be given an image. Your task is to describe the architecture given in the image. If asked about a specific part of the image, only return the information in the specific part. Describe with as detail as possible, describing each component and the relationships between the components. Also explain what the components and relationships might mean.\"),\n",
        "    (\"human\", [\n",
        "        {\n",
        "                  \"type\": \"text\",\n",
        "                  \"text\": \"\"\"Focus on the following diagram: {diagram_info}\n",
        "\n",
        "                  The following text is in the image: {ocr_text}\"\"\"\n",
        "                  },\n",
        "              {\n",
        "                  \"type\": \"image_url\",\n",
        "                  \"image_url\": {\n",
        "                      \"url\": \"{imgurl}\"\n",
        "                      }\n",
        "                  }\n",
        "              ]),\n",
        "  ])\n",
        "  title = diagram_info\n",
        "  if(isinstance(diagram_info, dict)):\n",
        "    if \"topic\" in diagram_info:\n",
        "      title = diagram_info[\"topic\"]\n",
        "  prompt_value = prompt.invoke({\"imgurl\": imgurl, \"diagram_info\": title, \"ocr_text\": ocr_text})\n",
        "  try:\n",
        "    answer = llm.invoke(prompt_value)\n",
        "  except:\n",
        "    print(\"Error in describe_architecture_flowchart_diagram. Trying again\")\n",
        "    answer = describe_architecture_flowchart_diagram(imgurl, diagram_info, ocr_text)\n",
        "  return answer\n",
        "\n",
        "@tool\n",
        "def describe_generic(imgurl, diagram_info, ocr_text):\n",
        "  \"\"\"Returns a detailed description of a generic type of visual information. Use this tool when the other tools for describing visual information in the image do not entirely fit.\"\"\"\n",
        "  prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You will be given an image. Your task is to describe the visual context of the image in as much detail as possible. Describe every component of the image and every relationship between the components in detail. If asked about a specific part of the image, only return the information in the specific part.\"),\n",
        "    (\"human\", [\n",
        "        {\n",
        "                  \"type\": \"text\",\n",
        "                  \"text\": \"\"\"Focus on the following content: {diagram_info}\n",
        "\n",
        "                  The following text is in the image: {ocr_text}\"\"\"\n",
        "                  },\n",
        "              {\n",
        "                  \"type\": \"image_url\",\n",
        "                  \"image_url\": {\n",
        "                      \"url\": \"{imgurl}\"\n",
        "                      }\n",
        "                  }\n",
        "              ]),\n",
        "  ])\n",
        "  title = diagram_info\n",
        "  if(isinstance(diagram_info, dict)):\n",
        "    if \"topic\" in diagram_info:\n",
        "      title = diagram_info[\"topic\"]\n",
        "  prompt_value = prompt.invoke({\"imgurl\": imgurl, \"diagram_info\": title, \"ocr_text\": ocr_text})\n",
        "  try:\n",
        "    answer = llm.invoke(prompt_value)\n",
        "  except:\n",
        "    print(\"Error in describe_generic. Trying again\")\n",
        "    answer = describe_generic(imgurl, diagram_info, ocr_text)\n",
        "  return answer\n",
        "\n",
        "@tool\n",
        "def get_bars_count(imgurl, diagram_info):\n",
        "  \"\"\"Returns the amount of bars for any specified bar chart in an image. Always use this tool first when processing bar charts.\"\"\"\n",
        "  prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You will be given an image with a bar chart in it. Your task is to identify the amount bars in the chart. If asked about a specific bar chart of the image, only return the amount of bars in the specified bar chart.\"),\n",
        "    (\"human\", [\n",
        "        {\n",
        "                  \"type\": \"text\",\n",
        "                  \"text\": \"Focus on the following chart: {diagram_info}\"\n",
        "                  },\n",
        "              {\n",
        "                  \"type\": \"image_url\",\n",
        "                  \"image_url\": {\n",
        "                      \"url\": \"{imgurl}\"\n",
        "                      }\n",
        "                  }\n",
        "              ]),\n",
        "  ])\n",
        "  title = diagram_info\n",
        "  if(isinstance(diagram_info, dict)):\n",
        "    if \"topic\" in diagram_info:\n",
        "      title = diagram_info[\"topic\"]\n",
        "  prompt_value = prompt.invoke({\"imgurl\": imgurl, \"diagram_info\": title})\n",
        "  try:\n",
        "    answer = llm.invoke(prompt_value)\n",
        "  except:\n",
        "    print(\"Error in get_bars_count. Trying again\")\n",
        "    answer = get_bars_count(imgurl, diagram_info)\n",
        "  return answer\n",
        "\n",
        "@tool\n",
        "def get_color_meaning(imgurl, diagram_info):\n",
        "  \"\"\"Returns the meaning of the colors used in a diagram. Always use this tool before describing the diagram.\"\"\"\n",
        "  prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You will be given an image with a diagram in it. Your task is to identify the meaning of the colors in the diagram. If asked about a specific diagram of the image, only return the information for the colors in the specified diagram.\"),\n",
        "    (\"human\", [\n",
        "        {\n",
        "                  \"type\": \"text\",\n",
        "                  \"text\": \"Focus on the following diagram: {diagram_info}\"\n",
        "                  },\n",
        "              {\n",
        "                  \"type\": \"image_url\",\n",
        "                  \"image_url\": {\n",
        "                      \"url\": \"{imgurl}\"\n",
        "                      }\n",
        "                  }\n",
        "              ]),\n",
        "  ])\n",
        "  title = diagram_info\n",
        "  if(isinstance(diagram_info, dict)):\n",
        "    if \"topic\" in diagram_info:\n",
        "      title = diagram_info[\"topic\"]\n",
        "  prompt_value = prompt.invoke({\"imgurl\": imgurl, \"diagram_info\": title})\n",
        "  try:\n",
        "    answer = llm.invoke(prompt_value)\n",
        "  except:\n",
        "    print(\"Error in get_color_meaning. Trying again\")\n",
        "    answer = get_color_meaning(imgurl, diagram_info)\n",
        "  return answer\n",
        "\n",
        "@tool\n",
        "def get_word_length(word):\n",
        "  \"\"\"Returns the length of a word\"\"\"\n",
        "  return len(word)\n",
        "\n",
        "@tool\n",
        "def get_factor(initial_value, result_value):\n",
        "  \"\"\"Returns the factor that the initial value has to be multiplied with, in order to equal the result value\"\"\"\n",
        "  return result_value/initial_value\n",
        "\n",
        "@tool\n",
        "def get_difference(val1, val2):\n",
        "  \"\"\"Returns the difference between two numbers\"\"\"\n",
        "  return abs(val1 - val2)"
      ],
      "metadata": {
        "id": "SB8Wvo8Kj_fV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pytesseract\n",
        "import requests\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import io\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def recognize_text(imgurl):\n",
        "  response = requests.get(imgurl)\n",
        "  img = Image.open(io.BytesIO(response.content))\n",
        "\n",
        "  img_array = np.asarray(img)\n",
        "\n",
        "  kernelSize = 5\n",
        "  maxKernel = cv2.getStructuringElement(cv2.MORPH_RECT, (kernelSize, kernelSize))\n",
        "  localMax = cv2.morphologyEx(img_array, cv2.MORPH_CLOSE, maxKernel, None, None, 1, cv2.BORDER_REFLECT101)\n",
        "\n",
        "  gainDivision = np.where(localMax == 0, 0, (img_array/localMax))\n",
        "\n",
        "  gainDivision = np.clip((255 * gainDivision), 0, 255)\n",
        "\n",
        "  gainDivision = gainDivision.astype(\"uint8\")\n",
        "\n",
        "  grayscaleImage = cv2.cvtColor(gainDivision, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "  _, binaryImage = cv2.threshold(grayscaleImage, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
        "\n",
        "  grayscaleImage = cv2.cvtColor(gainDivision, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "  _, binaryImage = cv2.threshold(grayscaleImage, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
        "\n",
        "  text = pytesseract.image_to_string(binaryImage, lang=\"eng+deu\")\n",
        "  return text"
      ],
      "metadata": {
        "id": "Dh8nMeAIkWRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_description(image_url):\n",
        "  tools = [get_diagrams_from_slide, get_word_length, get_factor, get_difference, get_bars_count, get_color_meaning, describe_bar_chart, describe_pie_chart, describe_timeline_chart, describe_table, describe_architecture_flowchart_diagram, describe_generic] #, recognize_text]\n",
        "  llm_with_tools = llm.bind_tools(tools)\n",
        "\n",
        "  prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a powerful visual assistant who can extract visual information from presentation slides with the help of some specified tools\"),\n",
        "    (\"human\", [\n",
        "              {\n",
        "                  \"type\": \"text\",\n",
        "                  \"text\": \"\"\"Based on the following data, answer the question: {question}\n",
        "\n",
        "                  Data: {data},\n",
        "                  Text in the image: {ocr_text}\"\"\"\n",
        "              }\n",
        "              ]),\n",
        "    MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
        "  ])\n",
        "\n",
        "  agent = (\n",
        "    {\n",
        "        \"question\": lambda x: x[\"question\"],\n",
        "        \"data\" : lambda x: x[\"data\"],\n",
        "        \"ocr_text\": lambda x: x[\"ocr_text\"],\n",
        "        \"agent_scratchpad\": lambda x: format_to_openai_tool_messages(\n",
        "            x[\"intermediate_steps\"]\n",
        "        )\n",
        "    }\n",
        "    | prompt\n",
        "    | llm_with_tools\n",
        "    | OpenAIToolsAgentOutputParser()\n",
        "  )\n",
        "\n",
        "  text = recognize_text(image_url)\n",
        "\n",
        "  agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
        "  try:\n",
        "    agent_executor_result = agent_executor.invoke({\"question\": \"Describe the given image in detail\", \"data\": image_url, \"ocr_text\": text})\n",
        "  except:\n",
        "    print(\"Error running agent\")\n",
        "    agent_executor_result = generate_description(image_url)\n",
        "\n",
        "  return agent_executor_result[\"output\"]"
      ],
      "metadata": {
        "id": "gfe4-_nmj6YC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(api_key=userdata.get(\"OPENAI_API_KEY\"))\n",
        "\n",
        "def get_text_embedding(text):\n",
        "  response = client.embeddings.create(\n",
        "    input=text,\n",
        "    model=\"text-embedding-3-large\"\n",
        "    )\n",
        "  return response.data[0].embedding"
      ],
      "metadata": {
        "id": "ykBg4CuIt6e-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pymongo\n",
        "import requests\n",
        "from google.colab import userdata\n",
        "\n",
        "from pydrive2.auth import GoogleAuth\n",
        "from pydrive2.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "metadata": {
        "id": "1B74Hu5L12kI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "metadata": {
        "id": "VQI1JqAc2TTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_images_folder = drive.ListFile({'q': \"mimeType = 'application/vnd.google-apps.folder' and title = 'output-images'\" }).GetList()[0]\n",
        "\n",
        "output_images_files = drive.ListFile({'q': \"'\"+ output_images_folder['id'] + \"' in parents and fileExtension = 'png'\"}).GetList()"
      ],
      "metadata": {
        "id": "x42eM5tWZIyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mongo_client = pymongo.MongoClient(userdata.get('CLUSTER_BACHELOR_CLUSTER_CONNECTION_STRING'))\n",
        "\n",
        "db = mongo_client[\"approach-3\"]\n",
        "collection = db[\"embeddings\"]"
      ],
      "metadata": {
        "id": "JQPt-wwBBI6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_list = []\n",
        "\n",
        "for file in output_images_files:\n",
        "  file.InsertPermission({\n",
        "      'type': 'anyone',\n",
        "      'value': 'anyone',\n",
        "      'role': 'reader'\n",
        "  })\n",
        "  image_url = \"https://drive.usercontent.google.com/download?id=\" + file['id'] + \"&authuser=0\"\n",
        "  while(True):\n",
        "    print(\"Trying: \" + image_url)\n",
        "    try:\n",
        "      text = generate_description(image_url)\n",
        "    except:\n",
        "      print(\"Error in generate_description. Trying again\")\n",
        "      continue\n",
        "    break\n",
        "  embedding = get_text_embedding(text)\n",
        "  data_list.append({\"embedding\": embedding, \"text\": text})"
      ],
      "metadata": {
        "id": "8GmEut6IZNXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collection.insert_many(data_list)"
      ],
      "metadata": {
        "id": "J501dlb3Mtel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_similar_descriptions(query):\n",
        "  embedded_query = get_text_embedding(query)\n",
        "\n",
        "  similarity_search_pipeline = [\n",
        "    {\n",
        "      \"$vectorSearch\": {\n",
        "        \"index\": \"approach_3_index\",\n",
        "        \"path\": \"embedding\",\n",
        "        \"queryVector\": embedded_query,\n",
        "        \"numCandidates\": 150,\n",
        "        \"limit\": 5\n",
        "      }\n",
        "    },\n",
        "    {\n",
        "      \"$project\": {\n",
        "        \"_id\": 0,\n",
        "        \"embedding\": 1,\n",
        "        \"text\": 1,\n",
        "        \"score\": { \"$meta\": \"vectorSearchScore\" }\n",
        "      }\n",
        "    }\n",
        "  ]\n",
        "\n",
        "  result = db.embeddings.aggregate(similarity_search_pipeline)\n",
        "\n",
        "  text_list = []\n",
        "  for chunk in result:\n",
        "    text_list.append(chunk['text'])\n",
        "\n",
        "  return text_list"
      ],
      "metadata": {
        "id": "YIjnmnGycjoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.prompts.chat import MessagesPlaceholder\n",
        "from langchain_core.messages.system import SystemMessage\n",
        "from langchain_core.messages.human import HumanMessage\n",
        "\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "llm = ChatOpenAI(api_key=OPENAI_API_KEY, model=\"gpt-4o\")\n",
        "\n",
        "def get_answer(query):\n",
        "  similar_descriptions = get_similar_descriptions(query)\n",
        "\n",
        "  inner_text = \"\"\n",
        "  for text in similar_descriptions:\n",
        "    inner_text += f\"{text}\\n\\n\"\n",
        "\n",
        "  inner_text = inner_text.replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
        "\n",
        "  prompt_text = []\n",
        "  prompt_text.append(\n",
        "      {\"type\": \"text\",\n",
        "      \"text\": f\"\"\"Based on the following data, answer the question: {{question}}\n",
        "\n",
        "      Data: {inner_text}\n",
        "      \"\"\"}\n",
        "      )\n",
        "\n",
        "  prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a powerful assistant who can answer questions based on information in the given text.\"),\n",
        "    (\"human\", prompt_text)\n",
        "  ])\n",
        "\n",
        "  llm_chain = prompt | llm\n",
        "  result = llm_chain.invoke(query).content\n",
        "\n",
        "  return result"
      ],
      "metadata": {
        "id": "AgEuvwiJc-jr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from rouge_score import rouge_scorer\n",
        "from bert_score import score\n",
        "\n",
        "def create_scores():\n",
        "  df_bleu = pd.read_csv(\"/content/drive/MyDrive/bachelor-thesis/results/bleu.csv\")\n",
        "  df_rouge1 = pd.read_csv(\"/content/drive/MyDrive/bachelor-thesis/results/rouge-1.csv\")\n",
        "  df_rouge2 = pd.read_csv(\"/content/drive/MyDrive/bachelor-thesis/results/rouge-2.csv\")\n",
        "  df_rougel = pd.read_csv(\"/content/drive/MyDrive/bachelor-thesis/results/rouge-l.csv\")\n",
        "  df_bertscore = pd.read_csv(\"/content/drive/MyDrive/bachelor-thesis/results/bertscore.csv\")\n",
        "\n",
        "  for i, question in enumerate(df_bleu[\"question\"]):\n",
        "    if i >= 50:\n",
        "      break\n",
        "\n",
        "    while(True):\n",
        "      print(\"Question \" + str(i))\n",
        "      try:\n",
        "        answer = get_answer(question)\n",
        "      except Exception as error:\n",
        "        print(\"Error in getting answer. Trying again. Error:\")\n",
        "        print(error)\n",
        "        continue\n",
        "      break\n",
        "\n",
        "    df_bleu[\"approach-3-text\"][i] = answer\n",
        "    df_rouge1[\"approach-3-text\"][i] = answer\n",
        "    df_rouge2[\"approach-3-text\"][i] = answer\n",
        "    df_rougel[\"approach-3-text\"][i] = answer\n",
        "    df_bertscore[\"approach-3-text\"][i] = answer\n",
        "\n",
        "    split_answer = answer.split()\n",
        "\n",
        "    reference_answer = df_bleu[\"reference-text\"][i]\n",
        "    split_reference_answer = reference_answer.split()\n",
        "\n",
        "    bleu_score = sentence_bleu([split_reference_answer], split_answer)\n",
        "\n",
        "    _rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeLsum'], use_stemmer=True)\n",
        "    rouge_scores = _rouge_scorer.score(reference_answer, answer)\n",
        "\n",
        "    rouge1 = rouge_scores['rouge1'].precision\n",
        "    rouge2 = rouge_scores['rouge2'].precision\n",
        "    rougeLsum = rouge_scores['rougeLsum'].precision\n",
        "\n",
        "    bertscore_tensor, _, _ = score(cands=[answer], refs=[reference_answer], lang=\"en\")\n",
        "    bertscore = bertscore_tensor.numpy()[0]\n",
        "\n",
        "\n",
        "    df_bleu[\"approach-3-score\"][i] = bleu_score\n",
        "    df_rouge1['approach-3-score'][i] = rouge1\n",
        "    df_rouge2['approach-3-score'][i] = rouge2\n",
        "    df_rougel['approach-3-score'][i] = rougeLsum\n",
        "    df_bertscore['approach-3-score'][i] = bertscore\n",
        "\n",
        "    df_bleu.to_csv(\"/content/drive/MyDrive/bachelor-thesis/results/bleu.csv\")\n",
        "    df_rouge1.to_csv(\"/content/drive/MyDrive/bachelor-thesis/results/rouge-1.csv\")\n",
        "    df_rouge2.to_csv(\"/content/drive/MyDrive/bachelor-thesis/results/rouge-2.csv\")\n",
        "    df_rougel.to_csv(\"/content/drive/MyDrive/bachelor-thesis/results/rouge-l.csv\")\n",
        "    df_bertscore.to_csv(\"/content/drive/MyDrive/bachelor-thesis/results/bertscore.csv\")"
      ],
      "metadata": {
        "id": "8QddnVzbd1Pg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_scores()"
      ],
      "metadata": {
        "id": "FPIAESPveInE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}